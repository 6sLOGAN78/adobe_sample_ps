{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0289cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/logan78/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/logan78/.local/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/logan78/.local/lib/python3.10/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/logan78/.local/lib/python3.10/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/logan78/.local/lib/python3.10/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/logan78/.local/lib/python3.10/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/logan78/.local/lib/python3.10/site-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Original: 1280x1920 | Resized: 683x1024 | Padding: 341px bottom\n",
      "âœ… Encoder runtime: 0.504s | Embedding shape: (1, 256, 64, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/logan78/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Decoder runtime: 0.073s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.099s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.075s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.080s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.072s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.074s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.139s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.100s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.101s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.088s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.086s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.094s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.095s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.121s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.099s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.109s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.097s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.266s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.178s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.112s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.136s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.286s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.145s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.109s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.084s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.080s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.082s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.097s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.085s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.084s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.080s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.082s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.079s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.073s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.078s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.084s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.086s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.085s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.076s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.082s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.080s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.082s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.082s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.075s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.078s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.082s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.073s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.073s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.072s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.072s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.079s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.070s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.085s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.078s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.085s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.075s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.076s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.073s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.082s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.074s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.079s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.083s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.076s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.074s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.075s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.075s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.080s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n",
      "ðŸ§© Decoder runtime: 0.081s\n",
      "Cropping mask: valid 170x256, pad_y=85, pad_x=0\n"
     ]
    }
   ],
   "source": [
    "#with mobile_sam model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "from mobile_sam import sam_model_registry\n",
    "from mobile_sam.utils.transforms import ResizeLongestSide\n",
    "import time\n",
    "\n",
    "# ---------------- Load ONNX Encoder & Decoder ----------------\n",
    "encoder = ort.InferenceSession(\"mobile_sam_encoder.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "decoder = ort.InferenceSession(\"mobile_sam_decoder.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# ---------------- Load Prompt Encoder ----------------\n",
    "sam = sam_model_registry[\"vit_t\"](checkpoint=\"/home/logan78/Desktop/adobe/models/mobile_sam.pt\")\n",
    "sam.to(\"cpu\")\n",
    "prompt_encoder = sam.prompt_encoder\n",
    "transform = ResizeLongestSide(1024)\n",
    "\n",
    "# ---------------- Load Image ----------------\n",
    "image_path = \"/home/logan78/Desktop/adobe/sean-pollock-PhYq704ffdA-unsplash.jpg\"\n",
    "orig = cv2.imread(image_path)\n",
    "H, W = orig.shape[:2]\n",
    "rgb = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# --- Resize + Pad to 1024 ---\n",
    "resized = transform.apply_image(rgb).astype(np.float32)\n",
    "tensor = torch.as_tensor(resized).permute(2, 0, 1)[None]  # 1x3xHresxWres\n",
    "\n",
    "# Pad to 1024Ã—1024\n",
    "padded = torch.zeros((1, 3, 1024, 1024), dtype=torch.float32)\n",
    "padded[:, :, :tensor.shape[2], :tensor.shape[3]] = tensor\n",
    "padded = padded / 255.0\n",
    "\n",
    "print(f\"ðŸ§  Original: {H}x{W} | Resized: {tensor.shape[2]}x{tensor.shape[3]} | Padding: {(1024-tensor.shape[2])}px bottom\")\n",
    "\n",
    "# ---------------- Run Encoder ----------------\n",
    "t0 = time.time()\n",
    "image_embedding = encoder.run(None, {\"image\": padded.numpy()})[0]\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Encoder runtime: {t1 - t0:.3f}s | Embedding shape: {image_embedding.shape}\")\n",
    "\n",
    "# ---------------- Mouse Click Interaction ----------------\n",
    "clicks = []\n",
    "\n",
    "def mouse(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        clicks.clear()\n",
    "        clicks.append((x, y))\n",
    "\n",
    "cv2.namedWindow(\"MobileSAM Live\")\n",
    "cv2.setMouseCallback(\"MobileSAM Live\", mouse)\n",
    "\n",
    "# ---------------- Live Loop ----------------\n",
    "while True:\n",
    "    frame = orig.copy()\n",
    "\n",
    "    if clicks:\n",
    "        px, py = clicks[0]\n",
    "\n",
    "        # Convert click â†’ model coordinates\n",
    "        pt = np.array([[px, py]])\n",
    "        pt = torch.as_tensor(transform.apply_coords(pt, (H, W)), dtype=torch.float32)[None]  # 1x1x2\n",
    "        labels = torch.ones(1, dtype=torch.int64)[None]  # 1x1 positive point\n",
    "\n",
    "        # Prompt Encoder\n",
    "        with torch.no_grad():\n",
    "            sparse, dense = prompt_encoder(points=(pt, labels), boxes=None, masks=None)\n",
    "        sparse = sparse.detach().cpu().numpy()\n",
    "        dense = dense.detach().cpu().numpy()\n",
    "\n",
    "        # ---------------- Run Decoder ----------------\n",
    "        t2 = time.time()\n",
    "        mask, iou = decoder.run(None, {\n",
    "            \"image_embedding\": image_embedding,\n",
    "            \"sparse_prompt\": sparse,\n",
    "            \"dense_prompt\": dense\n",
    "        })\n",
    "        t3 = time.time()\n",
    "        print(f\"ðŸ§© Decoder runtime: {t3 - t2:.3f}s\")\n",
    "\n",
    "        # ---------------- Correct Cropping ----------------\n",
    "        mask = mask[0][0]  # (256,256)\n",
    "        new_h, new_w = tensor.shape[2] // 4, tensor.shape[3] // 4\n",
    "        pad_y = (1024 - tensor.shape[2]) // 4\n",
    "        pad_x = (1024 - tensor.shape[3]) // 4\n",
    "        print(f\"Cropping mask: valid {new_h}x{new_w}, pad_y={pad_y}, pad_x={pad_x}\")\n",
    "\n",
    "        mask = mask[:new_h, :new_w]\n",
    "\n",
    "        # Resize back to original\n",
    "        mask = cv2.resize(mask, (W, H))\n",
    "        mask_bin = (mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Overlay mask in green\n",
    "        frame[mask_bin == 1] = (0, 255, 0)\n",
    "\n",
    "    cv2.imshow(\"MobileSAM Live\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to exit\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cbe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ONNX models loaded successfully!\n",
      "âœ… Prompt encoder loaded.\n",
      "ðŸ§  Image Info â€” Original: 1280x1920, Resized: 683x1024, Pad bottom: 341\n",
      "âœ… Encoder runtime: 4.343s | Embedding shape: (1, 256, 64, 64)\n",
      "ðŸ–±ï¸ Clicked at: (445, 576)\n",
      "ðŸ§© Decoder runtime: 0.062s | IOU: 0.646\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) /io/opencv/modules/imgproc/src/resize.cpp:4086: error: (-215:Assertion failed) func != 0 in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m new_h, new_w \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    119\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask[:new_h, :new_w]\n\u001b[0;32m--> 120\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m mask_bin \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Green overlay\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.12.0) /io/opencv/modules/imgproc/src/resize.cpp:4086: error: (-215:Assertion failed) func != 0 in function 'resize'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import time\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# âš™ï¸ Load ONNX Encoder & Decoder\n",
    "# -----------------------------------------------------------\n",
    "encoder_path = \"/home/logan78/Desktop/adobe/onnx_fp16/sam_vit_b_encoder_fp16.onnx\"\n",
    "decoder_path = \"/home/logan78/Desktop/adobe/onnx_fp16/sam_vit_b_decoder_fp16.onnx\"\n",
    "\n",
    "encoder = ort.InferenceSession(encoder_path, providers=[\"CPUExecutionProvider\"])\n",
    "decoder = ort.InferenceSession(decoder_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"âœ… ONNX models loaded successfully!\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# âš™ï¸ Load Prompt Encoder (PyTorch)\n",
    "# -----------------------------------------------------------\n",
    "checkpoint = \"/home/logan78/Desktop/adobe/models/sam_vit_b_01ec64.pth\"\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=checkpoint)\n",
    "sam.to(\"cpu\").eval()\n",
    "prompt_encoder = sam.prompt_encoder\n",
    "transform = ResizeLongestSide(1024)\n",
    "print(\"âœ… Prompt encoder loaded.\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ðŸ–¼ï¸ Load Image\n",
    "# -----------------------------------------------------------\n",
    "image_path = \"/home/logan78/Desktop/adobe/sean-pollock-PhYq704ffdA-unsplash.jpg\"\n",
    "orig = cv2.imread(image_path)\n",
    "H, W = orig.shape[:2]\n",
    "rgb = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize and pad to 1024Ã—1024\n",
    "resized = transform.apply_image(rgb).astype(np.float32)\n",
    "tensor = torch.as_tensor(resized).permute(2, 0, 1)[None]\n",
    "padded = torch.zeros((1, 3, 1024, 1024), dtype=torch.float32)\n",
    "padded[:, :, :tensor.shape[2], :tensor.shape[3]] = tensor\n",
    "padded /= 255.0\n",
    "\n",
    "print(f\"ðŸ§  Image Info â€” Original: {H}x{W}, Resized: {tensor.shape[2]}x{tensor.shape[3]}, Pad bottom: {1024 - tensor.shape[2]}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ðŸš€ Run Encoder\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.time()\n",
    "image_embedding = encoder.run(None, {\"image\": padded.numpy().astype(np.float16)})[0]\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Encoder runtime: {t1 - t0:.3f}s | Embedding shape: {image_embedding.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ðŸ–±ï¸ Mouse Click Setup\n",
    "# -----------------------------------------------------------\n",
    "clicks = []\n",
    "\n",
    "def mouse(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        clicks.clear()\n",
    "        clicks.append((x, y))\n",
    "        print(f\"ðŸ–±ï¸ Clicked at: ({x}, {y})\")\n",
    "\n",
    "cv2.namedWindow(\"SAM ViT-B Live\")\n",
    "cv2.setMouseCallback(\"SAM ViT-B Live\", mouse)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ðŸ§  Auto dtype function (for safety)\n",
    "# -----------------------------------------------------------\n",
    "def auto_cast(name, arr):\n",
    "    \"\"\"Ensure correct dtype based on ONNX input expectation.\"\"\"\n",
    "    for inp in decoder.get_inputs():\n",
    "        if inp.name == name:\n",
    "            if inp.type == \"tensor(float16)\":\n",
    "                return arr.astype(np.float16)\n",
    "            else:\n",
    "                return arr.astype(np.float32)\n",
    "    return arr\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ðŸŽ¯ Main Interactive Loop\n",
    "# -----------------------------------------------------------\n",
    "while True:\n",
    "    frame = orig.copy()\n",
    "\n",
    "    if clicks:\n",
    "        px, py = clicks[0]\n",
    "\n",
    "        # Convert click â†’ SAM input coords\n",
    "        pt = np.array([[px, py]])\n",
    "        pt = torch.as_tensor(transform.apply_coords(pt, (H, W)), dtype=torch.float32)[None]\n",
    "        labels = torch.ones(1, dtype=torch.int64)[None]\n",
    "\n",
    "        # Prompt encoder â†’ sparse + dense\n",
    "        with torch.no_grad():\n",
    "            sparse, dense = prompt_encoder(points=(pt, labels), boxes=None, masks=None)\n",
    "        sparse = sparse.detach().cpu().numpy()\n",
    "        dense = dense.detach().cpu().numpy()\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Run Decoder (with dtype auto fix)\n",
    "        # ---------------------------------------------------\n",
    "        t2 = time.time()\n",
    "        mask, iou = decoder.run(None, {\n",
    "            \"image_embedding\": auto_cast(\"image_embedding\", image_embedding),\n",
    "            \"sparse_prompt\": auto_cast(\"sparse_prompt\", sparse),\n",
    "            \"dense_prompt\": auto_cast(\"dense_prompt\", dense)\n",
    "        })\n",
    "        t3 = time.time()\n",
    "        print(f\"ðŸ§© Decoder runtime: {t3 - t2:.3f}s | IOU: {iou[0][0]:.3f}\")\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Postprocess mask\n",
    "        # ---------------------------------------------------\n",
    "        mask = mask[0][0]  # (256,256)\n",
    "        new_h, new_w = tensor.shape[2] // 4, tensor.shape[3] // 4\n",
    "        mask = mask[:new_h, :new_w]\n",
    "        mask = cv2.resize(mask, (W, H))\n",
    "        mask_bin = (mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Green overlay\n",
    "        frame[mask_bin == 1] = (0, 255, 0)\n",
    "\n",
    "    cv2.imshow(\"SAM ViT-B Live\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7bac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
